{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battery RUL Prediction - CatBoost GPU Training\n",
    "\n",
    "This notebook trains a CatBoost regression model to predict Battery Remaining Useful Life (RUL) using Kaggle GPU (P100).\n",
    "\n",
    "**Dataset**: Battery telemetry data in Parquet format\n",
    "**Target**: RUL in days\n",
    "**Model**: CatBoost with GPU acceleration\n",
    "\n",
    "**Author**: Battery RUL Prediction System\n",
    "**Version**: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m\u00d7\u001b[0m \u001b[32minstalling build dependencies for catboost\u001b[0m did not run successfully.\n",
      "  \u001b[31m\u2502\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m\u2570\u2500>\u001b[0m \u001b[31m[103 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Collecting setuptools>=64.0\n",
      "  \u001b[31m   \u001b[0m   Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting wheel\n",
      "  \u001b[31m   \u001b[0m   Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting jupyterlab\n",
      "  \u001b[31m   \u001b[0m   Using cached jupyterlab-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting conan<=1.59,>=1.57\n",
      "  \u001b[31m   \u001b[0m   Using cached conan-1.59.0.tar.gz (780 kB)\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m Collecting requests<3.0.0,>=2.25 (from conan<=1.59,>=1.57)\n",
      "  \u001b[31m   \u001b[0m   Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting urllib3<1.27,>=1.26.6 (from conan<=1.59,>=1.57)\n",
      "  \u001b[31m   \u001b[0m   Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting colorama<0.5.0,>=0.3.3 (from conan<=1.59,>=1.57)\n",
      "  \u001b[31m   \u001b[0m   Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting PyYAML<=6.0,>=3.11 (from conan<=1.59,>=1.57)\n",
      "  \u001b[31m   \u001b[0m   Using cached PyYAML-6.0.tar.gz (124 kB)\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[31m\u00d7\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m\u2502\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m\u2570\u2500>\u001b[0m \u001b[31m[67 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m /tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m         License :: OSI Approved :: MIT License\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m writing lib/PyYAML.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m writing dependency_links to lib/PyYAML.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m writing top-level names to lib/PyYAML.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 143, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 301, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 317, in run_setup\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"<string>\", line 288, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 115, in setup\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_commands\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 1102, in run_command\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/command/egg_info.py\", line 312, in run\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     self.find_sources()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/command/egg_info.py\", line 320, in find_sources\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     mm.run()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/command/egg_info.py\", line 543, in run\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     self.add_defaults()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/command/egg_info.py\", line 581, in add_defaults\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     sdist.add_defaults(self)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/command/sdist.py\", line 109, in add_defaults\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     super().add_defaults()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/sdist.py\", line 245, in add_defaults\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     self._add_defaults_ext()\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/sdist.py\", line 330, in _add_defaults_ext\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     self.filelist.extend(build_ext.get_source_files())\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"<string>\", line 204, in get_source_files\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-07c7y7ow/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 131, in __getattr__\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     raise AttributeError(attr)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m AttributeError: cython_sources\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[31mERROR: Failed to build 'PyYAML' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'catboost' when installing build dependencies for catboost\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q catboost==1.2 pyarrow==15.0.0 pandas==2.1.4 scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "GPU Available: False (Devices: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error 35 (CUDA driver version is insufficient for CUDA runtime version) ignored while obtaining device count\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ML libraries\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "\tfrom catboost.utils import get_gpu_device_count\n",
    "\tgpu_count = get_gpu_device_count()\n",
    "\tprint(f\"GPU Available: {gpu_count > 0} (Devices: {gpu_count})\")\n",
    "except:\n",
    "\tprint(\"GPU Available: Unknown (will be tested during training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading from Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /kaggle/working/data/parquet\n",
      "Directory exists: False\n"
     ]
    }
   ],
   "source": [
    "# Define paths (adjust based on Kaggle dataset structure)\n",
    "DATA_DIR = Path('/kaggle/input/battery-rul-parquet')  # Update with actual Kaggle dataset name\n",
    "\n",
    "# Alternative: if data is in working directory\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = Path('/kaggle/working/data/parquet')\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Directory exists: {DATA_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/data/parquet/master/battery.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load master data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading master data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df_battery \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaster\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbattery.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_location \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaster\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatteries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_battery)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    273\u001b[0m         path_or_handle,\n\u001b[1;32m    274\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    278\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/data/parquet/master/battery.parquet'"
     ]
    }
   ],
   "source": [
    "# Load master data\n",
    "print(\"Loading master data...\")\n",
    "df_battery = pd.read_parquet(DATA_DIR / 'master' / 'battery.parquet')\n",
    "df_location = pd.read_parquet(DATA_DIR / 'master' / 'location.parquet')\n",
    "\n",
    "print(f\"Batteries: {len(df_battery)}\")\n",
    "print(f\"Locations: {len(df_location)}\")\n",
    "\n",
    "# Display sample\n",
    "df_battery.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load telemetry data\n",
    "print(\"Loading telemetry data...\")\n",
    "df_raw_telemetry = pd.read_parquet(DATA_DIR / 'telemetry' / 'raw_telemetry.parquet')\n",
    "df_calc_telemetry = pd.read_parquet(DATA_DIR / 'telemetry' / 'calc_telemetry.parquet')\n",
    "\n",
    "print(f\"Raw telemetry records: {len(df_raw_telemetry):,}\")\n",
    "print(f\"Calculated telemetry records: {len(df_calc_telemetry):,}\")\n",
    "\n",
    "# Convert timestamps\n",
    "df_raw_telemetry['ts'] = pd.to_datetime(df_raw_telemetry['ts'])\n",
    "df_calc_telemetry['ts'] = pd.to_datetime(df_calc_telemetry['ts'])\n",
    "\n",
    "print(f\"\\nDate range: {df_raw_telemetry['ts'].min()} to {df_raw_telemetry['ts'].max()}\")\n",
    "df_raw_telemetry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RUL predictions (ground truth labels)\n",
    "print(\"Loading RUL predictions...\")\n",
    "df_rul = pd.read_parquet(DATA_DIR / 'ml' / 'rul_predictions.parquet')\n",
    "df_rul['prediction_time'] = pd.to_datetime(df_rul['prediction_time'])\n",
    "\n",
    "print(f\"RUL records: {len(df_rul):,}\")\n",
    "print(f\"\\nRUL statistics:\")\n",
    "print(df_rul['rul_days'].describe())\n",
    "\n",
    "df_rul.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature store (pre-aggregated features)\n",
    "print(\"Loading feature store...\")\n",
    "df_features = pd.read_parquet(DATA_DIR / 'ml' / 'feature_store.parquet')\n",
    "df_features['window_end'] = pd.to_datetime(df_features['window_end'])\n",
    "\n",
    "print(f\"Feature store records: {len(df_features):,}\")\n",
    "print(f\"\\nFeatures available: {df_features.columns.tolist()}\")\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features with RUL labels\n",
    "print(\"Merging features with RUL labels...\")\n",
    "\n",
    "# Match by battery_id and nearest timestamp\n",
    "df_train = pd.merge_asof(\n",
    "    df_features.sort_values('window_end'),\n",
    "    df_rul.sort_values('prediction_time'),\n",
    "    left_on='window_end',\n",
    "    right_on='prediction_time',\n",
    "    by='battery_id',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('1 hour')\n",
    ")\n",
    "\n",
    "# Remove rows without RUL labels\n",
    "df_train = df_train.dropna(subset=['rul_days'])\n",
    "\n",
    "print(f\"Training samples after merge: {len(df_train):,}\")\n",
    "print(f\"Batteries in training set: {df_train['battery_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "voltage_features = ['v_mean', 'v_std', 'v_min', 'v_max', 'v_range']\n",
    "temperature_features = ['t_mean', 't_std', 't_min', 't_max', 't_delta_from_ambient']\n",
    "resistance_features = ['r_internal_latest', 'r_internal_trend']\n",
    "operational_features = ['discharge_cycles_count', 'ah_throughput', 'time_at_high_temp_pct']\n",
    "\n",
    "# Combine all features\n",
    "feature_cols = (\n",
    "    voltage_features + \n",
    "    temperature_features + \n",
    "    resistance_features + \n",
    "    operational_features\n",
    ")\n",
    "\n",
    "# Verify all features exist\n",
    "missing_features = [f for f in feature_cols if f not in df_train.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "    feature_cols = [f for f in feature_cols if f in df_train.columns]\n",
    "\n",
    "print(f\"\\nFeatures selected for training ({len(feature_cols)}):\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features\n",
    "print(\"Creating derived features...\")\n",
    "\n",
    "# Voltage health indicator\n",
    "df_train['v_health_score'] = (\n",
    "    (df_train['v_mean'] - 11.5) / (13.65 - 11.5)  # Normalize to 0-1 range\n",
    ").clip(0, 1)\n",
    "\n",
    "# Temperature stress indicator\n",
    "df_train['t_stress_score'] = (\n",
    "    (df_train['t_max'] - 25) / 20  # Higher temp = more stress\n",
    ").clip(0, 1)\n",
    "\n",
    "# Resistance degradation rate\n",
    "if 'r_internal_latest' in df_train.columns and 'r_internal_trend' in df_train.columns:\n",
    "    df_train['r_degradation_rate'] = df_train['r_internal_trend'] / df_train['r_internal_latest']\n",
    "    feature_cols.append('r_degradation_rate')\n",
    "\n",
    "# Usage intensity\n",
    "if 'ah_throughput' in df_train.columns and 'discharge_cycles_count' in df_train.columns:\n",
    "    df_train['usage_intensity'] = df_train['ah_throughput'] / (df_train['discharge_cycles_count'] + 1)\n",
    "    feature_cols.append('usage_intensity')\n",
    "\n",
    "# Add derived features\n",
    "feature_cols.extend(['v_health_score', 't_stress_score'])\n",
    "\n",
    "print(f\"Total features after engineering: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "print(f\"\\nMissing values per feature:\")\n",
    "missing_counts = df_train[feature_cols].isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Fill missing values with median\n",
    "for col in feature_cols:\n",
    "    if df_train[col].isnull().any():\n",
    "        df_train[col].fillna(df_train[col].median(), inplace=True)\n",
    "\n",
    "print(f\"\\nData shape after cleaning: {df_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUL distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df_train['rul_days'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('RUL (days)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('RUL Distribution')\n",
    "axes[0].axvline(df_train['rul_days'].median(), color='red', linestyle='--', label=f'Median: {df_train[\"rul_days\"].median():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot(df_train['rul_days'])\n",
    "axes[1].set_ylabel('RUL (days)')\n",
    "axes[1].set_title('RUL Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/rul_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"RUL Statistics:\")\n",
    "print(df_train['rul_days'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlations with RUL\n",
    "correlations = df_train[feature_cols + ['rul_days']].corr()['rul_days'].drop('rul_days').sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlations.plot(kind='barh', color=['red' if x < 0 else 'green' for x in correlations])\n",
    "plt.xlabel('Correlation with RUL')\n",
    "plt.title('Feature Correlations with RUL')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/feature_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 features correlated with RUL:\")\n",
    "print(correlations.abs().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key features scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "key_features = ['v_mean', 't_max', 'r_internal_latest', 'discharge_cycles_count']\n",
    "for idx, feat in enumerate(key_features):\n",
    "    if feat in df_train.columns:\n",
    "        row, col = idx // 2, idx % 2\n",
    "        axes[row, col].scatter(df_train[feat], df_train['rul_days'], alpha=0.5, s=10)\n",
    "        axes[row, col].set_xlabel(feat)\n",
    "        axes[row, col].set_ylabel('RUL (days)')\n",
    "        axes[row, col].set_title(f'RUL vs {feat}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/rul_scatter_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_train[feature_cols].copy()\n",
    "y = df_train['rul_days'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeature value ranges:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split by RUL bins to ensure balanced distribution\n",
    "rul_bins = pd.cut(y, bins=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=rul_bins\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nRUL distribution in training set:\")\n",
    "print(y_train.describe())\n",
    "print(f\"\\nRUL distribution in test set:\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CatBoost GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CatBoost pools\n",
    "train_pool = Pool(X_train, y_train)\n",
    "test_pool = Pool(X_test, y_test)\n",
    "\n",
    "print(\"CatBoost pools created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure CatBoost model with GPU\n",
    "model = CatBoostRegressor(\n",
    "    # GPU Configuration\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    iterations=2000,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3,\n",
    "    \n",
    "    # Loss function\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='MAE',\n",
    "    \n",
    "    # Regularization\n",
    "    random_strength=1,\n",
    "    bagging_temperature=1,\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_rounds=100,\n",
    "    use_best_model=True,\n",
    "    \n",
    "    # Output\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"CatBoost model configured:\")\n",
    "print(f\"  Task type: GPU\")\n",
    "print(f\"  Iterations: 2000\")\n",
    "print(f\"  Learning rate: 0.05\")\n",
    "print(f\"  Tree depth: 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=test_pool,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "print(f\"Best iteration: {model.get_best_iteration()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MAE:  {train_mae:.2f} days\")\n",
    "print(f\"  RMSE: {train_rmse:.2f} days\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MAE:  {test_mae:.2f} days\")\n",
    "print(f\"  RMSE: {test_rmse:.2f} days\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nOverfitting Check:\")\n",
    "print(f\"  MAE gap:  {abs(test_mae - train_mae):.2f} days\")\n",
    "print(f\"  RMSE gap: {abs(test_rmse - train_rmse):.2f} days\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy analysis\n",
    "test_errors = np.abs(y_test - y_pred_test)\n",
    "\n",
    "print(\"\\nPrediction Error Analysis:\")\n",
    "print(f\"  Mean error: {test_errors.mean():.2f} days\")\n",
    "print(f\"  Median error: {test_errors.median():.2f} days\")\n",
    "print(f\"  90th percentile error: {test_errors.quantile(0.9):.2f} days\")\n",
    "print(f\"  Max error: {test_errors.max():.2f} days\")\n",
    "\n",
    "# Accuracy within thresholds\n",
    "within_7_days = (test_errors <= 7).mean() * 100\n",
    "within_30_days = (test_errors <= 30).mean() * 100\n",
    "within_60_days = (test_errors <= 60).mean() * 100\n",
    "\n",
    "print(f\"\\nPrediction Accuracy:\")\n",
    "print(f\"  Within 7 days:  {within_7_days:.1f}%\")\n",
    "print(f\"  Within 30 days: {within_30_days:.1f}%\")\n",
    "print(f\"  Within 60 days: {within_60_days:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual RUL (days)')\n",
    "axes[0].set_ylabel('Predicted RUL (days)')\n",
    "axes[0].set_title(f'Predicted vs Actual RUL (Test Set)\\nMAE: {test_mae:.2f} days, R\u00b2: {test_r2:.4f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.5, s=20)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted RUL (days)')\n",
    "axes[1].set_ylabel('Residuals (days)')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/prediction_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(test_errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Absolute Error (days)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Prediction Errors')\n",
    "axes[0].axvline(test_mae, color='red', linestyle='--', label=f'MAE: {test_mae:.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Residual (days)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "axes[1].axvline(0, color='red', linestyle='--', label='Zero error')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/error_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model.get_feature_importance()\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Top 15):\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# Save to CSV\n",
    "importance_df.to_csv('/kaggle/working/feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_n = 20\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Top {top_n} Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in multiple formats\n",
    "model_dir = Path('/kaggle/working')\n",
    "\n",
    "# 1. Native CatBoost format (.cbm)\n",
    "model_path_cbm = model_dir / 'rul_model.cbm'\n",
    "model.save_model(str(model_path_cbm))\n",
    "print(f\"Model saved (CatBoost): {model_path_cbm}\")\n",
    "\n",
    "# 2. ONNX format for deployment\n",
    "try:\n",
    "    model_path_onnx = model_dir / 'rul_model.onnx'\n",
    "    model.save_model(\n",
    "        str(model_path_onnx),\n",
    "        format=\"onnx\",\n",
    "        export_parameters={\n",
    "            'onnx_domain': 'ai.catboost',\n",
    "            'onnx_model_version': 1,\n",
    "            'onnx_doc_string': 'Battery RUL Prediction Model'\n",
    "        }\n",
    "    )\n",
    "    print(f\"Model saved (ONNX): {model_path_onnx}\")\n",
    "except Exception as e:\n",
    "    print(f\"ONNX export failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'CatBoostRegressor',\n",
    "    'task': 'Battery RUL Prediction',\n",
    "    'target': 'rul_days',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'training_time_seconds': training_time,\n",
    "    \n",
    "    # Data info\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': feature_cols,\n",
    "    'num_features': len(feature_cols),\n",
    "    \n",
    "    # Hyperparameters\n",
    "    'hyperparameters': {\n",
    "        'iterations': model.get_param('iterations'),\n",
    "        'learning_rate': model.get_param('learning_rate'),\n",
    "        'depth': model.get_param('depth'),\n",
    "        'l2_leaf_reg': model.get_param('l2_leaf_reg'),\n",
    "    },\n",
    "    \n",
    "    # Performance metrics\n",
    "    'metrics': {\n",
    "        'train': {\n",
    "            'mae': float(train_mae),\n",
    "            'rmse': float(train_rmse),\n",
    "            'r2': float(train_r2)\n",
    "        },\n",
    "        'test': {\n",
    "            'mae': float(test_mae),\n",
    "            'rmse': float(test_rmse),\n",
    "            'r2': float(test_r2)\n",
    "        },\n",
    "        'accuracy_thresholds': {\n",
    "            'within_7_days_pct': float(within_7_days),\n",
    "            'within_30_days_pct': float(within_30_days),\n",
    "            'within_60_days_pct': float(within_60_days)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Feature importance\n",
    "    'top_10_features': importance_df.head(10).to_dict('records')\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = model_dir / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nModel metadata saved: {metadata_path}\")\n",
    "print(f\"\\nMetadata summary:\")\n",
    "print(f\"  Features: {metadata['num_features']}\")\n",
    "print(f\"  Training samples: {metadata['training_samples']:,}\")\n",
    "print(f\"  Test MAE: {metadata['metrics']['test']['mae']:.2f} days\")\n",
    "print(f\"  Test R\u00b2: {metadata['metrics']['test']['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment package\n",
    "import zipfile\n",
    "\n",
    "deployment_package = model_dir / 'rul_model_deployment.zip'\n",
    "\n",
    "with zipfile.ZipFile(deployment_package, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add model files\n",
    "    zipf.write(model_path_cbm, 'rul_model.cbm')\n",
    "    if (model_dir / 'rul_model.onnx').exists():\n",
    "        zipf.write(model_dir / 'rul_model.onnx', 'rul_model.onnx')\n",
    "    \n",
    "    # Add metadata and documentation\n",
    "    zipf.write(metadata_path, 'model_metadata.json')\n",
    "    zipf.write(model_dir / 'feature_importance.csv', 'feature_importance.csv')\n",
    "    \n",
    "    # Add visualizations\n",
    "    for viz in ['rul_distribution.png', 'feature_correlations.png', \n",
    "                'prediction_analysis.png', 'feature_importance.png', 'error_distribution.png']:\n",
    "        viz_path = model_dir / viz\n",
    "        if viz_path.exists():\n",
    "            zipf.write(viz_path, viz)\n",
    "\n",
    "print(f\"\\nDeployment package created: {deployment_package}\")\n",
    "print(f\"Package size: {deployment_package.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "BATTERY RUL PREDICTION MODEL - TRAINING REPORT\n",
    "{'='*80}\n",
    "\n",
    "Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Training Duration: {training_time:.1f} seconds ({training_time/60:.1f} minutes)\n",
    "\n",
    "DATASET INFORMATION\n",
    "-------------------\n",
    "Training samples: {len(X_train):,}\n",
    "Test samples: {len(X_test):,}\n",
    "Total features: {len(feature_cols)}\n",
    "RUL range: {y.min():.1f} - {y.max():.1f} days\n",
    "RUL mean: {y.mean():.1f} days\n",
    "RUL std: {y.std():.1f} days\n",
    "\n",
    "MODEL CONFIGURATION\n",
    "-------------------\n",
    "Algorithm: CatBoostRegressor (GPU)\n",
    "Iterations: {model.get_param('iterations')}\n",
    "Learning rate: {model.get_param('learning_rate')}\n",
    "Tree depth: {model.get_param('depth')}\n",
    "L2 regularization: {model.get_param('l2_leaf_reg')}\n",
    "Best iteration: {model.get_best_iteration()}\n",
    "\n",
    "PERFORMANCE METRICS\n",
    "-------------------\n",
    "Training Set:\n",
    "  MAE:  {train_mae:.2f} days\n",
    "  RMSE: {train_rmse:.2f} days\n",
    "  R\u00b2:   {train_r2:.4f}\n",
    "\n",
    "Test Set:\n",
    "  MAE:  {test_mae:.2f} days\n",
    "  RMSE: {test_rmse:.2f} days\n",
    "  R\u00b2:   {test_r2:.4f}\n",
    "\n",
    "Prediction Accuracy:\n",
    "  Within 7 days:  {within_7_days:.1f}%\n",
    "  Within 30 days: {within_30_days:.1f}%\n",
    "  Within 60 days: {within_60_days:.1f}%\n",
    "\n",
    "TOP 10 IMPORTANT FEATURES\n",
    "-------------------------\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in importance_df.head(10).iterrows():\n",
    "    report += f\"{row['feature']:30s} {row['importance']:8.2f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "OUTPUT FILES\n",
    "------------\n",
    "\u2713 rul_model.cbm (CatBoost format)\n",
    "\u2713 rul_model.onnx (ONNX format - if available)\n",
    "\u2713 model_metadata.json (Complete model information)\n",
    "\u2713 feature_importance.csv (Feature rankings)\n",
    "\u2713 rul_model_deployment.zip (Deployment package)\n",
    "\u2713 Visualization plots (PNG)\n",
    "\n",
    "DEPLOYMENT INSTRUCTIONS\n",
    "-----------------------\n",
    "1. Download outputs:\n",
    "   kaggle kernels output khiwnitithadachot/kaggle-notebook-optimized -p ./model\n",
    "\n",
    "2. Load model in production:\n",
    "   from catboost import CatBoostRegressor\n",
    "   model = CatBoostRegressor()\n",
    "   model.load_model('rul_model.cbm')\n",
    "\n",
    "3. Make predictions:\n",
    "   predictions = model.predict(X)\n",
    "\n",
    "{'='*80}\n",
    "TRAINING COMPLETED SUCCESSFULLY\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_path = model_dir / 'TRAINING_REPORT.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print(\"\\nOutput files available for download:\")\n",
    "print(\"=\"*60)\n",
    "for file in sorted(model_dir.glob('*')):\n",
    "    if file.is_file() and not file.name.startswith('.'):\n",
    "        size_mb = file.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {file.name:40s} {size_mb:8.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}