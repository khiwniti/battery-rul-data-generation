{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battery RUL Prediction - CatBoost GPU Training\n",
    "\n",
    "This notebook trains a CatBoost regression model to predict Battery Remaining Useful Life (RUL) using Kaggle GPU (P100).\n",
    "\n",
    "**Dataset**: Battery telemetry data in Parquet format\n",
    "**Target**: RUL in days\n",
    "**Model**: CatBoost with GPU acceleration\n",
    "\n",
    "**Author**: Battery RUL Prediction System\n",
    "**Version**: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q catboost==1.2 pyarrow==15.0.0 pandas==2.1.4 scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ML libraries\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"GPU Available: {CatBoostRegressor()._get_gpu_device_count() > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading from Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (adjust based on Kaggle dataset structure)\n",
    "DATA_DIR = Path('/kaggle/input/battery-rul-parquet')  # Update with actual Kaggle dataset name\n",
    "\n",
    "# Alternative: if data is in working directory\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = Path('/kaggle/working/data/parquet')\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Directory exists: {DATA_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load master data\n",
    "print(\"Loading master data...\")\n",
    "df_battery = pd.read_parquet(DATA_DIR / 'master' / 'battery.parquet')\n",
    "df_location = pd.read_parquet(DATA_DIR / 'master' / 'location.parquet')\n",
    "\n",
    "print(f\"Batteries: {len(df_battery)}\")\n",
    "print(f\"Locations: {len(df_location)}\")\n",
    "\n",
    "# Display sample\n",
    "df_battery.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load telemetry data\n",
    "print(\"Loading telemetry data...\")\n",
    "df_raw_telemetry = pd.read_parquet(DATA_DIR / 'telemetry' / 'raw_telemetry.parquet')\n",
    "df_calc_telemetry = pd.read_parquet(DATA_DIR / 'telemetry' / 'calc_telemetry.parquet')\n",
    "\n",
    "print(f\"Raw telemetry records: {len(df_raw_telemetry):,}\")\n",
    "print(f\"Calculated telemetry records: {len(df_calc_telemetry):,}\")\n",
    "\n",
    "# Convert timestamps\n",
    "df_raw_telemetry['ts'] = pd.to_datetime(df_raw_telemetry['ts'])\n",
    "df_calc_telemetry['ts'] = pd.to_datetime(df_calc_telemetry['ts'])\n",
    "\n",
    "print(f\"\\nDate range: {df_raw_telemetry['ts'].min()} to {df_raw_telemetry['ts'].max()}\")\n",
    "df_raw_telemetry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RUL predictions (ground truth labels)\n",
    "print(\"Loading RUL predictions...\")\n",
    "df_rul = pd.read_parquet(DATA_DIR / 'ml' / 'rul_predictions.parquet')\n",
    "df_rul['prediction_time'] = pd.to_datetime(df_rul['prediction_time'])\n",
    "\n",
    "print(f\"RUL records: {len(df_rul):,}\")\n",
    "print(f\"\\nRUL statistics:\")\n",
    "print(df_rul['rul_days'].describe())\n",
    "\n",
    "df_rul.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature store (pre-aggregated features)\n",
    "print(\"Loading feature store...\")\n",
    "df_features = pd.read_parquet(DATA_DIR / 'ml' / 'feature_store.parquet')\n",
    "df_features['window_end'] = pd.to_datetime(df_features['window_end'])\n",
    "\n",
    "print(f\"Feature store records: {len(df_features):,}\")\n",
    "print(f\"\\nFeatures available: {df_features.columns.tolist()}\")\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features with RUL labels\n",
    "print(\"Merging features with RUL labels...\")\n",
    "\n",
    "# Match by battery_id and nearest timestamp\n",
    "df_train = pd.merge_asof(\n",
    "    df_features.sort_values('window_end'),\n",
    "    df_rul.sort_values('prediction_time'),\n",
    "    left_on='window_end',\n",
    "    right_on='prediction_time',\n",
    "    by='battery_id',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('1 hour')\n",
    ")\n",
    "\n",
    "# Remove rows without RUL labels\n",
    "df_train = df_train.dropna(subset=['rul_days'])\n",
    "\n",
    "print(f\"Training samples after merge: {len(df_train):,}\")\n",
    "print(f\"Batteries in training set: {df_train['battery_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "voltage_features = ['v_mean', 'v_std', 'v_min', 'v_max', 'v_range']\n",
    "temperature_features = ['t_mean', 't_std', 't_min', 't_max', 't_delta_from_ambient']\n",
    "resistance_features = ['r_internal_latest', 'r_internal_trend']\n",
    "operational_features = ['discharge_cycles_count', 'ah_throughput', 'time_at_high_temp_pct']\n",
    "\n",
    "# Combine all features\n",
    "feature_cols = (\n",
    "    voltage_features + \n",
    "    temperature_features + \n",
    "    resistance_features + \n",
    "    operational_features\n",
    ")\n",
    "\n",
    "# Verify all features exist\n",
    "missing_features = [f for f in feature_cols if f not in df_train.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "    feature_cols = [f for f in feature_cols if f in df_train.columns]\n",
    "\n",
    "print(f\"\\nFeatures selected for training ({len(feature_cols)}):\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features\n",
    "print(\"Creating derived features...\")\n",
    "\n",
    "# Voltage health indicator\n",
    "df_train['v_health_score'] = (\n",
    "    (df_train['v_mean'] - 11.5) / (13.65 - 11.5)  # Normalize to 0-1 range\n",
    ").clip(0, 1)\n",
    "\n",
    "# Temperature stress indicator\n",
    "df_train['t_stress_score'] = (\n",
    "    (df_train['t_max'] - 25) / 20  # Higher temp = more stress\n",
    ").clip(0, 1)\n",
    "\n",
    "# Resistance degradation rate\n",
    "if 'r_internal_latest' in df_train.columns and 'r_internal_trend' in df_train.columns:\n",
    "    df_train['r_degradation_rate'] = df_train['r_internal_trend'] / df_train['r_internal_latest']\n",
    "    feature_cols.append('r_degradation_rate')\n",
    "\n",
    "# Usage intensity\n",
    "if 'ah_throughput' in df_train.columns and 'discharge_cycles_count' in df_train.columns:\n",
    "    df_train['usage_intensity'] = df_train['ah_throughput'] / (df_train['discharge_cycles_count'] + 1)\n",
    "    feature_cols.append('usage_intensity')\n",
    "\n",
    "# Add derived features\n",
    "feature_cols.extend(['v_health_score', 't_stress_score'])\n",
    "\n",
    "print(f\"Total features after engineering: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "print(f\"\\nMissing values per feature:\")\n",
    "missing_counts = df_train[feature_cols].isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Fill missing values with median\n",
    "for col in feature_cols:\n",
    "    if df_train[col].isnull().any():\n",
    "        df_train[col].fillna(df_train[col].median(), inplace=True)\n",
    "\n",
    "print(f\"\\nData shape after cleaning: {df_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUL distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df_train['rul_days'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('RUL (days)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('RUL Distribution')\n",
    "axes[0].axvline(df_train['rul_days'].median(), color='red', linestyle='--', label=f'Median: {df_train[\"rul_days\"].median():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot(df_train['rul_days'])\n",
    "axes[1].set_ylabel('RUL (days)')\n",
    "axes[1].set_title('RUL Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/rul_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"RUL Statistics:\")\n",
    "print(df_train['rul_days'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlations with RUL\n",
    "correlations = df_train[feature_cols + ['rul_days']].corr()['rul_days'].drop('rul_days').sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlations.plot(kind='barh', color=['red' if x < 0 else 'green' for x in correlations])\n",
    "plt.xlabel('Correlation with RUL')\n",
    "plt.title('Feature Correlations with RUL')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/feature_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 features correlated with RUL:\")\n",
    "print(correlations.abs().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key features scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "key_features = ['v_mean', 't_max', 'r_internal_latest', 'discharge_cycles_count']\n",
    "for idx, feat in enumerate(key_features):\n",
    "    if feat in df_train.columns:\n",
    "        row, col = idx // 2, idx % 2\n",
    "        axes[row, col].scatter(df_train[feat], df_train['rul_days'], alpha=0.5, s=10)\n",
    "        axes[row, col].set_xlabel(feat)\n",
    "        axes[row, col].set_ylabel('RUL (days)')\n",
    "        axes[row, col].set_title(f'RUL vs {feat}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/rul_scatter_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_train[feature_cols].copy()\n",
    "y = df_train['rul_days'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeature value ranges:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split by RUL bins to ensure balanced distribution\n",
    "rul_bins = pd.cut(y, bins=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=rul_bins\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nRUL distribution in training set:\")\n",
    "print(y_train.describe())\n",
    "print(f\"\\nRUL distribution in test set:\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CatBoost GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CatBoost pools\n",
    "train_pool = Pool(X_train, y_train)\n",
    "test_pool = Pool(X_test, y_test)\n",
    "\n",
    "print(\"CatBoost pools created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure CatBoost model with GPU\n",
    "model = CatBoostRegressor(\n",
    "    # GPU Configuration\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    iterations=2000,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3,\n",
    "    \n",
    "    # Loss function\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='MAE',\n",
    "    \n",
    "    # Regularization\n",
    "    random_strength=1,\n",
    "    bagging_temperature=1,\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_rounds=100,\n",
    "    use_best_model=True,\n",
    "    \n",
    "    # Output\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"CatBoost model configured:\")\n",
    "print(f\"  Task type: GPU\")\n",
    "print(f\"  Iterations: 2000\")\n",
    "print(f\"  Learning rate: 0.05\")\n",
    "print(f\"  Tree depth: 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=test_pool,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "print(f\"Best iteration: {model.get_best_iteration()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MAE:  {train_mae:.2f} days\")\n",
    "print(f\"  RMSE: {train_rmse:.2f} days\")\n",
    "print(f\"  R²:   {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MAE:  {test_mae:.2f} days\")\n",
    "print(f\"  RMSE: {test_rmse:.2f} days\")\n",
    "print(f\"  R²:   {test_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nOverfitting Check:\")\n",
    "print(f\"  MAE gap:  {abs(test_mae - train_mae):.2f} days\")\n",
    "print(f\"  RMSE gap: {abs(test_rmse - train_rmse):.2f} days\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy analysis\n",
    "test_errors = np.abs(y_test - y_pred_test)\n",
    "\n",
    "print(\"\\nPrediction Error Analysis:\")\n",
    "print(f\"  Mean error: {test_errors.mean():.2f} days\")\n",
    "print(f\"  Median error: {test_errors.median():.2f} days\")\n",
    "print(f\"  90th percentile error: {test_errors.quantile(0.9):.2f} days\")\n",
    "print(f\"  Max error: {test_errors.max():.2f} days\")\n",
    "\n",
    "# Accuracy within thresholds\n",
    "within_7_days = (test_errors <= 7).mean() * 100\n",
    "within_30_days = (test_errors <= 30).mean() * 100\n",
    "within_60_days = (test_errors <= 60).mean() * 100\n",
    "\n",
    "print(f\"\\nPrediction Accuracy:\")\n",
    "print(f\"  Within 7 days:  {within_7_days:.1f}%\")\n",
    "print(f\"  Within 30 days: {within_30_days:.1f}%\")\n",
    "print(f\"  Within 60 days: {within_60_days:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual RUL (days)')\n",
    "axes[0].set_ylabel('Predicted RUL (days)')\n",
    "axes[0].set_title(f'Predicted vs Actual RUL (Test Set)\\nMAE: {test_mae:.2f} days, R²: {test_r2:.4f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.5, s=20)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted RUL (days)')\n",
    "axes[1].set_ylabel('Residuals (days)')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/prediction_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(test_errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Absolute Error (days)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Prediction Errors')\n",
    "axes[0].axvline(test_mae, color='red', linestyle='--', label=f'MAE: {test_mae:.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Residual (days)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "axes[1].axvline(0, color='red', linestyle='--', label='Zero error')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/error_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model.get_feature_importance()\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Top 15):\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# Save to CSV\n",
    "importance_df.to_csv('/kaggle/working/feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_n = 20\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Top {top_n} Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in multiple formats\n",
    "model_dir = Path('/kaggle/working')\n",
    "\n",
    "# 1. Native CatBoost format (.cbm)\n",
    "model_path_cbm = model_dir / 'rul_model.cbm'\n",
    "model.save_model(str(model_path_cbm))\n",
    "print(f\"Model saved (CatBoost): {model_path_cbm}\")\n",
    "\n",
    "# 2. ONNX format for deployment\n",
    "try:\n",
    "    model_path_onnx = model_dir / 'rul_model.onnx'\n",
    "    model.save_model(\n",
    "        str(model_path_onnx),\n",
    "        format=\"onnx\",\n",
    "        export_parameters={\n",
    "            'onnx_domain': 'ai.catboost',\n",
    "            'onnx_model_version': 1,\n",
    "            'onnx_doc_string': 'Battery RUL Prediction Model'\n",
    "        }\n",
    "    )\n",
    "    print(f\"Model saved (ONNX): {model_path_onnx}\")\n",
    "except Exception as e:\n",
    "    print(f\"ONNX export failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'CatBoostRegressor',\n",
    "    'task': 'Battery RUL Prediction',\n",
    "    'target': 'rul_days',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'training_time_seconds': training_time,\n",
    "    \n",
    "    # Data info\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': feature_cols,\n",
    "    'num_features': len(feature_cols),\n",
    "    \n",
    "    # Hyperparameters\n",
    "    'hyperparameters': {\n",
    "        'iterations': model.get_param('iterations'),\n",
    "        'learning_rate': model.get_param('learning_rate'),\n",
    "        'depth': model.get_param('depth'),\n",
    "        'l2_leaf_reg': model.get_param('l2_leaf_reg'),\n",
    "    },\n",
    "    \n",
    "    # Performance metrics\n",
    "    'metrics': {\n",
    "        'train': {\n",
    "            'mae': float(train_mae),\n",
    "            'rmse': float(train_rmse),\n",
    "            'r2': float(train_r2)\n",
    "        },\n",
    "        'test': {\n",
    "            'mae': float(test_mae),\n",
    "            'rmse': float(test_rmse),\n",
    "            'r2': float(test_r2)\n",
    "        },\n",
    "        'accuracy_thresholds': {\n",
    "            'within_7_days_pct': float(within_7_days),\n",
    "            'within_30_days_pct': float(within_30_days),\n",
    "            'within_60_days_pct': float(within_60_days)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Feature importance\n",
    "    'top_10_features': importance_df.head(10).to_dict('records')\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = model_dir / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nModel metadata saved: {metadata_path}\")\n",
    "print(f\"\\nMetadata summary:\")\n",
    "print(f\"  Features: {metadata['num_features']}\")\n",
    "print(f\"  Training samples: {metadata['training_samples']:,}\")\n",
    "print(f\"  Test MAE: {metadata['metrics']['test']['mae']:.2f} days\")\n",
    "print(f\"  Test R²: {metadata['metrics']['test']['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment package\n",
    "import zipfile\n",
    "\n",
    "deployment_package = model_dir / 'rul_model_deployment.zip'\n",
    "\n",
    "with zipfile.ZipFile(deployment_package, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add model files\n",
    "    zipf.write(model_path_cbm, 'rul_model.cbm')\n",
    "    if (model_dir / 'rul_model.onnx').exists():\n",
    "        zipf.write(model_dir / 'rul_model.onnx', 'rul_model.onnx')\n",
    "    \n",
    "    # Add metadata and documentation\n",
    "    zipf.write(metadata_path, 'model_metadata.json')\n",
    "    zipf.write(model_dir / 'feature_importance.csv', 'feature_importance.csv')\n",
    "    \n",
    "    # Add visualizations\n",
    "    for viz in ['rul_distribution.png', 'feature_correlations.png', \n",
    "                'prediction_analysis.png', 'feature_importance.png', 'error_distribution.png']:\n",
    "        viz_path = model_dir / viz\n",
    "        if viz_path.exists():\n",
    "            zipf.write(viz_path, viz)\n",
    "\n",
    "print(f\"\\nDeployment package created: {deployment_package}\")\n",
    "print(f\"Package size: {deployment_package.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "BATTERY RUL PREDICTION MODEL - TRAINING REPORT\n",
    "{'='*80}\n",
    "\n",
    "Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Training Duration: {training_time:.1f} seconds ({training_time/60:.1f} minutes)\n",
    "\n",
    "DATASET INFORMATION\n",
    "-------------------\n",
    "Training samples: {len(X_train):,}\n",
    "Test samples: {len(X_test):,}\n",
    "Total features: {len(feature_cols)}\n",
    "RUL range: {y.min():.1f} - {y.max():.1f} days\n",
    "RUL mean: {y.mean():.1f} days\n",
    "RUL std: {y.std():.1f} days\n",
    "\n",
    "MODEL CONFIGURATION\n",
    "-------------------\n",
    "Algorithm: CatBoostRegressor (GPU)\n",
    "Iterations: {model.get_param('iterations')}\n",
    "Learning rate: {model.get_param('learning_rate')}\n",
    "Tree depth: {model.get_param('depth')}\n",
    "L2 regularization: {model.get_param('l2_leaf_reg')}\n",
    "Best iteration: {model.get_best_iteration()}\n",
    "\n",
    "PERFORMANCE METRICS\n",
    "-------------------\n",
    "Training Set:\n",
    "  MAE:  {train_mae:.2f} days\n",
    "  RMSE: {train_rmse:.2f} days\n",
    "  R²:   {train_r2:.4f}\n",
    "\n",
    "Test Set:\n",
    "  MAE:  {test_mae:.2f} days\n",
    "  RMSE: {test_rmse:.2f} days\n",
    "  R²:   {test_r2:.4f}\n",
    "\n",
    "Prediction Accuracy:\n",
    "  Within 7 days:  {within_7_days:.1f}%\n",
    "  Within 30 days: {within_30_days:.1f}%\n",
    "  Within 60 days: {within_60_days:.1f}%\n",
    "\n",
    "TOP 10 IMPORTANT FEATURES\n",
    "-------------------------\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in importance_df.head(10).iterrows():\n",
    "    report += f\"{row['feature']:30s} {row['importance']:8.2f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "OUTPUT FILES\n",
    "------------\n",
    "✓ rul_model.cbm (CatBoost format)\n",
    "✓ rul_model.onnx (ONNX format - if available)\n",
    "✓ model_metadata.json (Complete model information)\n",
    "✓ feature_importance.csv (Feature rankings)\n",
    "✓ rul_model_deployment.zip (Deployment package)\n",
    "✓ Visualization plots (PNG)\n",
    "\n",
    "DEPLOYMENT INSTRUCTIONS\n",
    "-----------------------\n",
    "1. Download outputs:\n",
    "   kaggle kernels output khiwnitithadachot/kaggle-notebook-optimized -p ./model\n",
    "\n",
    "2. Load model in production:\n",
    "   from catboost import CatBoostRegressor\n",
    "   model = CatBoostRegressor()\n",
    "   model.load_model('rul_model.cbm')\n",
    "\n",
    "3. Make predictions:\n",
    "   predictions = model.predict(X)\n",
    "\n",
    "{'='*80}\n",
    "TRAINING COMPLETED SUCCESSFULLY\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_path = model_dir / 'TRAINING_REPORT.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print(\"\\nOutput files available for download:\")\n",
    "print(\"=\"*60)\n",
    "for file in sorted(model_dir.glob('*')):\n",
    "    if file.is_file() and not file.name.startswith('.'):\n",
    "        size_mb = file.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {file.name:40s} {size_mb:8.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
