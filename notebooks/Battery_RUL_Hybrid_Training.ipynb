{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battery RUL Prediction - Complete End-to-End Training\n",
    "\n",
    "This notebook provides a complete workflow from data acquisition to trained model export.\n",
    "\n",
    "## üéØ What This Notebook Does\n",
    "\n",
    "1. **Data Acquisition** - Auto-loads dataset or generates if needed\n",
    "2. **Feature Engineering** - Creates ML-ready features\n",
    "3. **Model Training** - GPU-accelerated CatBoost\n",
    "4. **Evaluation** - Comprehensive metrics and visualizations\n",
    "5. **Export** - Multi-format model packaging\n",
    "\n",
    "## ‚è±Ô∏è Runtime\n",
    "\n",
    "- With uploaded dataset: **~30 minutes**\n",
    "- With auto-generation: **~45 minutes**\n",
    "\n",
    "## üîß Requirements\n",
    "\n",
    "1. **Enable GPU** (Settings ‚Üí Accelerator ‚Üí GPU P100/T4)\n",
    "2. **Enable Internet** (Settings ‚Üí Internet ‚Üí On)\n",
    "3. **Optional**: Upload battery-rul-parquet dataset for faster execution\n",
    "\n",
    "## üìä Expected Output\n",
    "\n",
    "- Trained CatBoost model (.cbm, .onnx)\n",
    "- Model metadata (metrics, features)\n",
    "- Feature importance rankings\n",
    "- Visualizations (5 plots)\n",
    "- Training report\n",
    "- Deployment package (.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"üîß Installing dependencies...\")\n",
    "print(\"(This may take 2-3 minutes)\\n\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q catboost==1.2 pyarrow==15.0.0 pandas==2.1.4 scikit-learn matplotlib seaborn\n",
    "\n",
    "# Also install data generation deps in case we need to generate\n",
    "!pip install -q numpy scipy pytz faker tqdm\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ML libraries\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    gpu_available = result.returncode == 0\n",
    "    print(f\"\\nüéÆ GPU Available: {gpu_available}\")\n",
    "    if gpu_available:\n",
    "        print(\"   GPU detected - Training will use GPU acceleration\")\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è  GPU status unknown - Will attempt GPU training anyway\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 2: Smart Data Acquisition\n",
    "\n",
    "This cell intelligently handles data loading:\n",
    "- **Option A**: If you uploaded a dataset, it loads from `/kaggle/input/`\n",
    "- **Option B**: If no dataset found, generates a small training set (7 days, 24 batteries)\n",
    "\n",
    "### To use uploaded dataset:\n",
    "1. Upload `battery-rul-parquet` as a Kaggle dataset\n",
    "2. Add it to this notebook (Add Data ‚Üí Your Datasets)\n",
    "3. The cell will automatically detect and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"=\"*80)\n",
    "print(\"DATA ACQUISITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for uploaded Parquet dataset\n",
    "KAGGLE_INPUT = Path('/kaggle/input')\n",
    "possible_paths = [\n",
    "    KAGGLE_INPUT / 'battery-rul-parquet',\n",
    "    KAGGLE_INPUT / 'battery-rul-training-data',\n",
    "    KAGGLE_INPUT / 'battery-rul-data'\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in possible_paths:\n",
    "    if path.exists():\n",
    "        dataset_path = path\n",
    "        print(f\"\\n‚úÖ Found uploaded dataset at: {path}\")\n",
    "        print(f\"   Using pre-generated data (fast mode)\")\n",
    "        break\n",
    "\n",
    "if dataset_path is None:\n",
    "    print(\"\\n‚ö†Ô∏è  No uploaded dataset found\")\n",
    "    print(\"   Will generate training data (takes ~15 minutes)\")\n",
    "    print(\"\\nüìä Generation Parameters:\")\n",
    "    print(\"   - Duration: 7 days\")\n",
    "    print(\"   - Batteries: 24 (1 string)\")\n",
    "    print(\"   - Sampling: 60 seconds\")\n",
    "    print(\"   - Expected records: ~10,080 telemetry samples\")\n",
    "    print(\"\\nüöÄ Starting data generation...\\n\")\n",
    "    \n",
    "    # Clone repository if needed\n",
    "    if not os.path.exists('battery-rul-data-generation'):\n",
    "        !git clone -q https://github.com/khiwniti/battery-rul-data-generation.git\n",
    "        print(\"‚úì Repository cloned\")\n",
    "    \n",
    "    os.chdir('battery-rul-data-generation')\n",
    "    \n",
    "    # Generate data\n",
    "    !python generate_battery_data.py --duration-days 7 --limit-batteries 24 --sampling-seconds 60 --output-dir ./output/training_data --seed 42\n",
    "    \n",
    "    dataset_path = Path('./output/training_data')\n",
    "    os.chdir('/kaggle/working')\n",
    "    print(\"\\n‚úÖ Data generation complete!\")\n",
    "\n",
    "print(f\"\\nüìÅ Dataset location: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine file format (Parquet or CSV)\n",
    "use_parquet = (dataset_path / 'telemetry' / 'raw_telemetry.parquet').exists()\n",
    "use_csv = (dataset_path / 'telemetry_jar_raw.csv.gz').exists()\n",
    "\n",
    "if use_parquet:\n",
    "    print(\"\\nüì¶ Loading from Parquet files...\")\n",
    "    \n",
    "    # Load master data\n",
    "    df_battery = pd.read_parquet(dataset_path / 'master' / 'battery.parquet')\n",
    "    df_location = pd.read_parquet(dataset_path / 'master' / 'location.parquet')\n",
    "    \n",
    "    # Load telemetry\n",
    "    df_raw_telemetry = pd.read_parquet(dataset_path / 'telemetry' / 'raw_telemetry.parquet')\n",
    "    df_calc_telemetry = pd.read_parquet(dataset_path / 'telemetry' / 'calc_telemetry.parquet')\n",
    "    \n",
    "    # Load RUL predictions\n",
    "    df_rul = pd.read_parquet(dataset_path / 'ml' / 'rul_predictions.parquet')\n",
    "    \n",
    "    # Load feature store if exists\n",
    "    feature_store_path = dataset_path / 'ml' / 'feature_store.parquet'\n",
    "    if feature_store_path.exists():\n",
    "        df_features = pd.read_parquet(feature_store_path)\n",
    "    else:\n",
    "        df_features = None\n",
    "        print(\"   ‚ö†Ô∏è  Feature store not found, will create from raw data\")\n",
    "    \n",
    "elif use_csv:\n",
    "    print(\"\\nüìÑ Loading from CSV files...\")\n",
    "    \n",
    "    # Load master data\n",
    "    df_battery = pd.read_csv(dataset_path / 'battery.csv')\n",
    "    df_location = pd.read_csv(dataset_path / 'location.csv')\n",
    "    \n",
    "    # Load telemetry\n",
    "    df_raw_telemetry = pd.read_csv(dataset_path / 'telemetry_jar_raw.csv.gz')\n",
    "    df_calc_telemetry = pd.read_csv(dataset_path / 'telemetry_jar_calc.csv')\n",
    "    \n",
    "    # Load RUL predictions\n",
    "    df_rul = pd.read_csv(dataset_path / 'rul_prediction.csv')\n",
    "    \n",
    "    # Load feature store if exists\n",
    "    feature_store_path = dataset_path / 'feature_store.csv.gz'\n",
    "    if feature_store_path.exists():\n",
    "        df_features = pd.read_csv(feature_store_path)\n",
    "    else:\n",
    "        df_features = None\n",
    "        print(\"   ‚ö†Ô∏è  Feature store not found, will create from raw data\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No valid data files found in dataset\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"   Batteries: {len(df_battery)}\")\n",
    "print(f\"   Locations: {len(df_location)}\")\n",
    "print(f\"   Telemetry records: {len(df_raw_telemetry):,}\")\n",
    "print(f\"   RUL predictions: {len(df_rul):,}\")\n",
    "if df_features is not None:\n",
    "    print(f\"   Feature store records: {len(df_features):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature store if it doesn't exist\n",
    "if df_features is None:\n",
    "    print(\"üî® Creating feature store from raw telemetry...\")\n",
    "    print(\"   (This may take a few minutes)\\n\")\n",
    "    \n",
    "    # Convert timestamps\n",
    "    ts_col = 'ts' if 'ts' in df_raw_telemetry.columns else 'timestamp'\n",
    "    df_raw_telemetry[ts_col] = pd.to_datetime(df_raw_telemetry[ts_col])\n",
    "    \n",
    "    # Create hourly features\n",
    "    df_features = df_raw_telemetry.groupby([\n",
    "        'battery_id',\n",
    "        pd.Grouper(key=ts_col, freq='1H')\n",
    "    ]).agg({\n",
    "        'voltage_v': ['mean', 'std', 'min', 'max'],\n",
    "        'temperature_c': ['mean', 'std', 'min', 'max'],\n",
    "        'resistance_mohm': ['mean', 'std'],\n",
    "        'current_a': ['mean', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    df_features.columns = ['battery_id', 'window_end'] + [\n",
    "        f\"{col[0].replace('_v','').replace('_c','').replace('_mohm','').replace('_a','')}_{col[1]}\"\n",
    "        for col in df_features.columns[2:]\n",
    "    ]\n",
    "    \n",
    "    # Rename to match expected schema\n",
    "    df_features = df_features.rename(columns={\n",
    "        'voltage_mean': 'v_mean',\n",
    "        'voltage_std': 'v_std',\n",
    "        'voltage_min': 'v_min',\n",
    "        'voltage_max': 'v_max',\n",
    "        'temperature_mean': 't_mean',\n",
    "        'temperature_std': 't_std',\n",
    "        'temperature_min': 't_min',\n",
    "        'temperature_max': 't_max',\n",
    "        'resistance_mean': 'r_internal_latest',\n",
    "        'resistance_std': 'r_internal_trend',\n",
    "        'current_mean': 'current_mean',\n",
    "        'current_max': 'current_max'\n",
    "    })\n",
    "    \n",
    "    # Add derived features\n",
    "    df_features['v_range'] = df_features['v_max'] - df_features['v_min']\n",
    "    df_features['t_delta_from_ambient'] = df_features['t_mean'] - 25.0  # Assume 25¬∞C ambient\n",
    "    \n",
    "    # Add operational features (approximations)\n",
    "    df_features['discharge_cycles_count'] = 0  # Would need more complex logic\n",
    "    df_features['ah_throughput'] = df_features['current_mean'] * 1.0  # Simplified\n",
    "    df_features['time_at_high_temp_pct'] = (df_features['t_max'] > 35).astype(float)\n",
    "    \n",
    "    print(f\"‚úÖ Feature store created: {len(df_features):,} records\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìä Feature Store Sample:\")\n",
    "print(df_features.head())\n",
    "print(f\"\\nFeatures available: {df_features.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Step 4: Feature Engineering & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Merge features with RUL labels\n",
    "print(\"\\nüîó Merging features with RUL labels...\")\n",
    "\n",
    "# Ensure timestamps are datetime\n",
    "df_features['window_end'] = pd.to_datetime(df_features['window_end'])\n",
    "rul_ts_col = 'prediction_time' if 'prediction_time' in df_rul.columns else 'timestamp'\n",
    "df_rul[rul_ts_col] = pd.to_datetime(df_rul[rul_ts_col])\n",
    "\n",
    "# Merge using nearest timestamp\n",
    "df_train = pd.merge_asof(\n",
    "    df_features.sort_values('window_end'),\n",
    "    df_rul.sort_values(rul_ts_col),\n",
    "    left_on='window_end',\n",
    "    right_on=rul_ts_col,\n",
    "    by='battery_id',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('2 hours')\n",
    ")\n",
    "\n",
    "# Remove rows without RUL labels\n",
    "df_train = df_train.dropna(subset=['rul_days'])\n",
    "\n",
    "print(f\"‚úì Training samples after merge: {len(df_train):,}\")\n",
    "print(f\"‚úì Batteries in training set: {df_train['battery_id'].nunique()}\")\n",
    "\n",
    "# Define feature columns\n",
    "voltage_features = ['v_mean', 'v_std', 'v_min', 'v_max', 'v_range']\n",
    "temperature_features = ['t_mean', 't_std', 't_min', 't_max', 't_delta_from_ambient']\n",
    "resistance_features = ['r_internal_latest', 'r_internal_trend']\n",
    "operational_features = ['discharge_cycles_count', 'ah_throughput', 'time_at_high_temp_pct']\n",
    "\n",
    "# Combine all features\n",
    "feature_cols = (\n",
    "    voltage_features + \n",
    "    temperature_features + \n",
    "    resistance_features + \n",
    "    operational_features\n",
    ")\n",
    "\n",
    "# Keep only features that exist\n",
    "feature_cols = [f for f in feature_cols if f in df_train.columns]\n",
    "\n",
    "print(f\"\\nüìä Features selected for training ({len(feature_cols)}):\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Create derived features\n",
    "print(\"\\nüî® Creating derived features...\")\n",
    "\n",
    "# Voltage health indicator\n",
    "df_train['v_health_score'] = (\n",
    "    (df_train['v_mean'] - 11.5) / (13.65 - 11.5)\n",
    ").clip(0, 1)\n",
    "\n",
    "# Temperature stress indicator\n",
    "df_train['t_stress_score'] = (\n",
    "    (df_train['t_max'] - 25) / 20\n",
    ").clip(0, 1)\n",
    "\n",
    "# Add derived features to list\n",
    "feature_cols.extend(['v_health_score', 't_stress_score'])\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nüßπ Cleaning data...\")\n",
    "for col in feature_cols:\n",
    "    if df_train[col].isnull().any():\n",
    "        df_train[col].fillna(df_train[col].median(), inplace=True)\n",
    "\n",
    "print(f\"‚úì Final dataset shape: {df_train.shape}\")\n",
    "print(f\"‚úì Total features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 5: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUL distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df_train['rul_days'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('RUL (days)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('RUL Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(df_train['rul_days'].median(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Median: {df_train[\"rul_days\"].median():.1f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].boxplot(df_train['rul_days'], patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue'))\n",
    "axes[1].set_ylabel('RUL (days)', fontsize=12)\n",
    "axes[1].set_title('RUL Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/rul_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä RUL Statistics:\")\n",
    "print(df_train['rul_days'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlations with RUL\n",
    "correlations = df_train[feature_cols + ['rul_days']].corr()['rul_days'].drop('rul_days').sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red' if x < 0 else 'green' for x in correlations]\n",
    "correlations.plot(kind='barh', color=colors, alpha=0.7)\n",
    "plt.xlabel('Correlation with RUL', fontsize=12)\n",
    "plt.title('Feature Correlations with RUL', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/feature_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Top 10 features correlated with RUL:\")\n",
    "print(correlations.abs().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 6: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_train[feature_cols].copy()\n",
    "y = df_train['rul_days'].copy()\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"   Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Target vector shape: {y.shape}\")\n",
    "print(f\"   RUL range: {y.min():.1f} - {y.max():.1f} days\")\n",
    "\n",
    "# Stratified split by RUL bins\n",
    "rul_bins = pd.cut(y, bins=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=rul_bins\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Train/Test Split Complete:\")\n",
    "print(f\"   Training set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 7: GPU-Accelerated Model Training\n",
    "\n",
    "Training CatBoost regression model with GPU acceleration.\n",
    "This typically takes 10-20 minutes depending on dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CatBoost pools\n",
    "train_pool = Pool(X_train, y_train)\n",
    "test_pool = Pool(X_test, y_test)\n",
    "\n",
    "print(\"‚úÖ CatBoost data pools created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configure CatBoost model with GPU\n",
    "model = CatBoostRegressor(\n",
    "    # GPU Configuration\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    iterations=2000,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3,\n",
    "    \n",
    "    # Loss function\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='MAE',\n",
    "    \n",
    "    # Regularization\n",
    "    random_strength=1,\n",
    "    bagging_temperature=1,\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_rounds=100,\n",
    "    use_best_model=True,\n",
    "    \n",
    "    # Output\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nüîß Model Configuration:\")\n",
    "print(f\"   Task type: GPU\")\n",
    "print(f\"   Iterations: 2000\")\n",
    "print(f\"   Learning rate: 0.05\")\n",
    "print(f\"   Tree depth: 8\")\n",
    "print(f\"   Early stopping: 100 rounds\")\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=test_pool,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Training completed!\")\n",
    "print(f\"   Time: {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "print(f\"   Best iteration: {model.get_best_iteration()}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìà Training Set:\")\n",
    "print(f\"   MAE:  {train_mae:.2f} days\")\n",
    "print(f\"   RMSE: {train_rmse:.2f} days\")\n",
    "print(f\"   R¬≤:   {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Test Set:\")\n",
    "print(f\"   MAE:  {test_mae:.2f} days\")\n",
    "print(f\"   RMSE: {test_rmse:.2f} days\")\n",
    "print(f\"   R¬≤:   {test_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ Overfitting Check:\")\n",
    "print(f\"   MAE gap:  {abs(test_mae - train_mae):.2f} days\")\n",
    "print(f\"   RMSE gap: {abs(test_rmse - train_rmse):.2f} days\")\n",
    "\n",
    "# Accuracy within thresholds\n",
    "test_errors = np.abs(y_test - y_pred_test)\n",
    "within_7_days = (test_errors <= 7).mean() * 100\n",
    "within_30_days = (test_errors <= 30).mean() * 100\n",
    "within_60_days = (test_errors <= 60).mean() * 100\n",
    "\n",
    "print(f\"\\nüéØ Prediction Accuracy:\")\n",
    "print(f\"   Within 7 days:  {within_7_days:.1f}%\")\n",
    "print(f\"   Within 30 days: {within_30_days:.1f}%\")\n",
    "print(f\"   Within 60 days: {within_60_days:.1f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Success criteria check\n",
    "print(\"\\n‚úÖ Success Criteria:\")\n",
    "if test_mae < 30:\n",
    "    print(f\"   ‚úì MAE < 30 days: PASS ({test_mae:.2f} days)\")\n",
    "else:\n",
    "    print(f\"   ‚úó MAE < 30 days: FAIL ({test_mae:.2f} days)\")\n",
    "\n",
    "if test_r2 > 0.85:\n",
    "    print(f\"   ‚úì R¬≤ > 0.85: PASS ({test_r2:.4f})\")\n",
    "else:\n",
    "    print(f\"   ‚úó R¬≤ > 0.85: FAIL ({test_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, s=20, color='steelblue')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual RUL (days)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted RUL (days)', fontsize=12)\n",
    "axes[0].set_title(f'Predicted vs Actual RUL\\nMAE: {test_mae:.2f} days, R¬≤: {test_r2:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.5, s=20, color='coral')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted RUL (days)', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals (days)', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/prediction_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 9: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model.get_feature_importance()\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üîç Feature Importance (Top 15):\")\n",
    "print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "importance_df.to_csv('/kaggle/working/feature_importance.csv', index=False)\n",
    "print(\"\\n‚úÖ Feature importance saved to feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_n = min(20, len(importance_df))\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title(f'Top {top_n} Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 10: Model Export\n",
    "\n",
    "Saving model in multiple formats for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL EXPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_dir = Path('/kaggle/working')\n",
    "\n",
    "# 1. Native CatBoost format (.cbm)\n",
    "model_path_cbm = model_dir / 'rul_model.cbm'\n",
    "model.save_model(str(model_path_cbm))\n",
    "print(f\"\\n‚úÖ Model saved (CatBoost): {model_path_cbm}\")\n",
    "print(f\"   Size: {model_path_cbm.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# 2. ONNX format for deployment\n",
    "try:\n",
    "    model_path_onnx = model_dir / 'rul_model.onnx'\n",
    "    model.save_model(\n",
    "        str(model_path_onnx),\n",
    "        format=\"onnx\",\n",
    "        export_parameters={\n",
    "            'onnx_domain': 'ai.catboost',\n",
    "            'onnx_model_version': 1,\n",
    "            'onnx_doc_string': 'Battery RUL Prediction Model'\n",
    "        }\n",
    "    )\n",
    "    print(f\"‚úÖ Model saved (ONNX): {model_path_onnx}\")\n",
    "    print(f\"   Size: {model_path_onnx.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  ONNX export not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'CatBoostRegressor',\n",
    "    'task': 'Battery RUL Prediction',\n",
    "    'target': 'rul_days',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'training_time_seconds': training_time,\n",
    "    \n",
    "    # Data info\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': feature_cols,\n",
    "    'num_features': len(feature_cols),\n",
    "    \n",
    "    # Hyperparameters\n",
    "    'hyperparameters': {\n",
    "        'iterations': model.get_param('iterations'),\n",
    "        'learning_rate': model.get_param('learning_rate'),\n",
    "        'depth': model.get_param('depth'),\n",
    "        'l2_leaf_reg': model.get_param('l2_leaf_reg'),\n",
    "    },\n",
    "    \n",
    "    # Performance metrics\n",
    "    'metrics': {\n",
    "        'train': {\n",
    "            'mae': float(train_mae),\n",
    "            'rmse': float(train_rmse),\n",
    "            'r2': float(train_r2)\n",
    "        },\n",
    "        'test': {\n",
    "            'mae': float(test_mae),\n",
    "            'rmse': float(test_rmse),\n",
    "            'r2': float(test_r2)\n",
    "        },\n",
    "        'accuracy_thresholds': {\n",
    "            'within_7_days_pct': float(within_7_days),\n",
    "            'within_30_days_pct': float(within_30_days),\n",
    "            'within_60_days_pct': float(within_60_days)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Feature importance\n",
    "    'top_10_features': importance_df.head(10).to_dict('records')\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = model_dir / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Metadata saved: {metadata_path}\")\n",
    "print(f\"   Test MAE: {metadata['metrics']['test']['mae']:.2f} days\")\n",
    "print(f\"   Test R¬≤: {metadata['metrics']['test']['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment package\n",
    "import zipfile\n",
    "\n",
    "deployment_package = model_dir / 'rul_model_deployment.zip'\n",
    "\n",
    "with zipfile.ZipFile(deployment_package, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add model files\n",
    "    zipf.write(model_path_cbm, 'rul_model.cbm')\n",
    "    if (model_dir / 'rul_model.onnx').exists():\n",
    "        zipf.write(model_dir / 'rul_model.onnx', 'rul_model.onnx')\n",
    "    \n",
    "    # Add metadata and documentation\n",
    "    zipf.write(metadata_path, 'model_metadata.json')\n",
    "    zipf.write(model_dir / 'feature_importance.csv', 'feature_importance.csv')\n",
    "    \n",
    "    # Add visualizations\n",
    "    for viz in ['rul_distribution.png', 'feature_correlations.png', \n",
    "                'prediction_analysis.png', 'feature_importance.png']:\n",
    "        viz_path = model_dir / viz\n",
    "        if viz_path.exists():\n",
    "            zipf.write(viz_path, viz)\n",
    "\n",
    "print(f\"\\n‚úÖ Deployment package created: {deployment_package}\")\n",
    "print(f\"   Size: {deployment_package.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 11: Model Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test model loading\n",
    "print(\"\\nüîç Testing model loading...\")\n",
    "test_model = CatBoostRegressor()\n",
    "test_model.load_model(str(model_path_cbm))\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "\n",
    "# Test prediction\n",
    "print(\"\\nüîç Testing prediction...\")\n",
    "sample_prediction = test_model.predict(X_test.iloc[:1])\n",
    "print(f\"‚úÖ Sample prediction: {sample_prediction[0]:.1f} days\")\n",
    "print(f\"   Actual RUL: {y_test.iloc[0]:.1f} days\")\n",
    "print(f\"   Error: {abs(sample_prediction[0] - y_test.iloc[0]):.1f} days\")\n",
    "\n",
    "# List all output files\n",
    "print(\"\\nüìÇ Output Files:\")\n",
    "print(\"=\"*80)\n",
    "for file in sorted(model_dir.glob('*')):\n",
    "    if file.is_file() and not file.name.startswith('.'):\n",
    "        size_mb = file.stat().st_size / 1024 / 1024\n",
    "        print(f\"   {file.name:40s} {size_mb:8.2f} MB\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéâ Model verification complete!\")\n",
    "print(\"\\n‚úÖ ALL OUTPUTS READY FOR DOWNLOAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Download Instructions\n",
    "\n",
    "### Method 1: Direct Download (Easiest)\n",
    "\n",
    "1. Click the **folder icon** (üìÅ) on the left sidebar\n",
    "2. Navigate to files in `/kaggle/working/`\n",
    "3. Click **three dots** (‚ãÆ) next to each file\n",
    "4. Select **\"Download\"**\n",
    "\n",
    "### Method 2: Kaggle API\n",
    "\n",
    "After saving this notebook version:\n",
    "\n",
    "```bash\n",
    "kaggle kernels output YOUR_USERNAME/battery-rul-training -p ./model\n",
    "```\n",
    "\n",
    "### Files to Download\n",
    "\n",
    "- `rul_model.cbm` - Main model file (1-5 MB)\n",
    "- `model_metadata.json` - Performance metrics\n",
    "- `feature_importance.csv` - Feature rankings\n",
    "- `rul_model_deployment.zip` - Complete package\n",
    "- `*.png` - Visualizations (5 files)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Download Model**: Use methods above\n",
    "2. **Test Locally**: Load and test model\n",
    "3. **Deploy**: Integrate with backend API\n",
    "4. **Monitor**: Track performance in production\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You've successfully trained a Battery RUL prediction model!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
